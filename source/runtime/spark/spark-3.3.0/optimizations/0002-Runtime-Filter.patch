From 9a5b6065dbf4f3277acce4bf0b82b5ecef431434 Mon Sep 17 00:00:00 2001
From: haojin <hao.jin@intel.com>
Date: Thu, 1 Sep 2022 17:10:34 +0800
Subject: [PATCH] Support more scenarios for runtime Filter

---
 .../read/SupportsEstimateStatistics.scala     |  24 ++++
 .../optimizer/InjectRuntimeFilter.scala       | 109 ++++++++++++++++--
 .../catalyst/plans/logical/LogicalPlan.scala  |   1 +
 .../apache/spark/sql/internal/SQLConf.scala   |  16 +++
 .../datasources/HadoopFsRelation.scala        |   5 +
 .../datasources/LogicalRelation.scala         |   9 +-
 .../PartitioningAwareFileIndex.scala          |   4 +-
 .../apache/spark/sql/sources/interfaces.scala |   2 +
 8 files changed, 159 insertions(+), 11 deletions(-)
 create mode 100644 sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsEstimateStatistics.scala

diff --git a/sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsEstimateStatistics.scala b/sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsEstimateStatistics.scala
new file mode 100644
index 0000000000..4b7d9f79e5
--- /dev/null
+++ b/sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/SupportsEstimateStatistics.scala
@@ -0,0 +1,24 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.sql.connector.read
+
+trait SupportsEstimateStatistics {
+
+  def estimateStats(): org.apache.spark.sql.catalyst.plans.logical.Statistics
+
+}
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala
index 134292ae30..b6f88e5240 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala
@@ -17,12 +17,14 @@
 
 package org.apache.spark.sql.catalyst.optimizer
 
+import scala.collection.mutable.ArrayBuffer
 import org.apache.spark.sql.catalyst.expressions._
 import org.apache.spark.sql.catalyst.expressions.aggregate.{AggregateExpression, BloomFilterAggregate, Complete}
 import org.apache.spark.sql.catalyst.planning.{ExtractEquiJoinKeys, PhysicalOperation}
 import org.apache.spark.sql.catalyst.plans.logical._
 import org.apache.spark.sql.catalyst.rules.Rule
 import org.apache.spark.sql.catalyst.trees.TreePattern.{INVOKE, JSON_TO_STRUCT, LIKE_FAMLIY, PYTHON_UDF, REGEXP_EXTRACT_FAMILY, REGEXP_REPLACE, SCALA_UDF}
+import org.apache.spark.sql.connector.read.SupportsEstimateStatistics
 import org.apache.spark.sql.internal.SQLConf
 import org.apache.spark.sql.types._
 
@@ -48,19 +50,29 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
       filterCreationSideExp: Expression,
       filterCreationSidePlan: LogicalPlan): LogicalPlan = {
     require(conf.runtimeFilterBloomFilterEnabled || conf.runtimeFilterSemiJoinReductionEnabled)
+    var newFilterCreationSidePlan = filterCreationSidePlan
+    var newFilterCreationSideExp = filterCreationSideExp
+    if (!isSelectiveFilterOverScan(filterCreationSidePlan)) {
+      val projectExpress = findOriginProject(
+        filterCreationSidePlan, filterCreationSideExp).
+        getOrElse().asInstanceOf[(LogicalPlan, Expression)]
+      newFilterCreationSidePlan = projectExpress._1
+      newFilterCreationSideExp = projectExpress._2
+    }
+
     if (conf.runtimeFilterBloomFilterEnabled) {
       injectBloomFilter(
         filterApplicationSideExp,
         filterApplicationSidePlan,
-        filterCreationSideExp,
-        filterCreationSidePlan
+        newFilterCreationSideExp,
+        newFilterCreationSidePlan
       )
     } else {
       injectInSubqueryFilter(
         filterApplicationSideExp,
         filterApplicationSidePlan,
-        filterCreationSideExp,
-        filterCreationSidePlan
+        newFilterCreationSideExp,
+        newFilterCreationSidePlan
       )
     }
   }
@@ -123,9 +135,41 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
           filters.exists(isLikelySelective)
       case _ => false
     }
+
     !plan.isStreaming && ret
   }
 
+  private def findOriginProject(
+      plan: LogicalPlan, expression: Expression):
+        Option[(LogicalPlan, Expression)] = {
+    val planWithExp: Option[ArrayBuffer[(LogicalPlan, Expression)]] = findPlanWithExp(
+      expression, plan, ArrayBuffer[(LogicalPlan, Expression)]())
+    if (planWithExp.isDefined) {
+      val path = planWithExp.getOrElse().asInstanceOf[ArrayBuffer[(LogicalPlan, Expression)]]
+      for (i <- (0 until path.size).reverse) {
+        if (path(i)._1.isInstanceOf[Project]) {
+          return Some(path(i))
+        }
+      }
+      None
+    }
+    None
+  }
+
+  private def isOriginProjectSelectiveFilterOverScan(
+      plan: LogicalPlan, expression: Expression): Boolean = {
+    if (conf.getConf(SQLConf.RUNTIME_FILTER_CREATION_SIDE_EXTEND_ENABLED)) {
+      val projectPlanAndExp = findOriginProject(plan, expression)
+      if (projectPlanAndExp.isDefined) {
+        val projectPlan = projectPlanAndExp.getOrElse().asInstanceOf[(LogicalPlan, Expression)]._1
+        return isSelectiveFilterOverScan(projectPlan)
+      } else {
+        return false
+      }
+    }
+    false
+  }
+
   private def isSimpleExpression(e: Expression): Boolean = {
     !e.containsAnyPattern(PYTHON_UDF, SCALA_UDF, INVOKE, JSON_TO_STRUCT, LIKE_FAMLIY,
       REGEXP_EXTRACT_FAMILY, REGEXP_REPLACE)
@@ -155,7 +199,22 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
       // creating a Bloom filter when the filter application side is very small, so using 0
       // as the byte size when the actual size is unknown can avoid regression by applying BF
       // on a small table.
-      if (scan.stats.sizeInBytes == defaultSizeInBytes) BigInt(0) else scan.stats.sizeInBytes
+      if (scan.stats.sizeInBytes == defaultSizeInBytes) {
+        if (conf.getConf(SQLConf.RUNTIME_FILTER_ESTIMATE_STATS_ENABLED)) {
+          if (scan.isInstanceOf[SupportsEstimateStatistics]) {
+            val estimateSizeInBytes =
+              scan.asInstanceOf[SupportsEstimateStatistics].estimateStats().sizeInBytes
+            if (estimateSizeInBytes == defaultSizeInBytes) BigInt(0) else estimateSizeInBytes
+          }
+          else {
+            BigInt(0)
+          }
+        } else {
+          BigInt(0)
+        }
+      } else {
+        scan.stats.sizeInBytes
+      }
     }).max
   }
 
@@ -182,9 +241,12 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
       filterApplicationSide: LogicalPlan,
       filterCreationSide: LogicalPlan,
       filterApplicationSideExp: Expression,
+      filterCreationSideSideExp: Expression,
       hint: JoinHint): Boolean = {
     findExpressionAndTrackLineageDown(filterApplicationSideExp,
-      filterApplicationSide).isDefined && isSelectiveFilterOverScan(filterCreationSide) &&
+      filterApplicationSide).isDefined &&
+      (isSelectiveFilterOverScan(filterCreationSide) ||
+        isOriginProjectSelectiveFilterOverScan(filterCreationSide, filterCreationSideSideExp)) &&
       (isProbablyShuffleJoin(filterApplicationSide, filterCreationSide, hint) ||
         probablyHasShuffle(filterApplicationSide)) &&
       satisfyByteSizeRequirement(filterApplicationSide)
@@ -235,6 +297,37 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
     }
   }
 
+  private def findPlanWithExp(
+    exp: Expression, plan: LogicalPlan, path: ArrayBuffer[(LogicalPlan, Expression)])
+        : Option[ArrayBuffer[(LogicalPlan, Expression)]] = {
+    if (exp.references.isEmpty) return None
+    plan match {
+      case p: Project =>
+        val aliases = getAliasMap(p)
+        val exp_aliases = replaceAlias(exp, aliases)
+        path += Tuple2(plan, exp_aliases)
+        findPlanWithExp(exp_aliases, p.child, path)
+      // we can unwrap only if there are row projections, and no aggregation operation
+      case a: Aggregate =>
+        val aliasMap = getAliasMap(a)
+        val exp_aliases = replaceAlias(exp, aliasMap)
+        path += Tuple2(plan, exp_aliases)
+        findPlanWithExp(exp_aliases, a.child, path)
+      case l: LeafNode if exp.references.subsetOf(l.outputSet) =>
+        path += Tuple2(plan, exp)
+        Some(path)
+      case other =>
+        path += Tuple2(plan, exp)
+        other.children.flatMap {
+          child => if (exp.references.subsetOf(child.outputSet)) {
+            findPlanWithExp(exp, child, path)
+          } else {
+            None
+          }
+        }.headOption
+    }
+  }
+
   def hasInSubquery(left: LogicalPlan, right: LogicalPlan, leftKey: Expression,
       rightKey: Expression): Boolean = {
     (left, right) match {
@@ -266,12 +359,12 @@ object InjectRuntimeFilter extends Rule[LogicalPlan] with PredicateHelper with J
             isSimpleExpression(l) && isSimpleExpression(r)) {
             val oldLeft = newLeft
             val oldRight = newRight
-            if (canPruneLeft(joinType) && filteringHasBenefit(left, right, l, hint)) {
+            if (canPruneLeft(joinType) && filteringHasBenefit(left, right, l, r, hint)) {
               newLeft = injectFilter(l, newLeft, r, right)
             }
             // Did we actually inject on the left? If not, try on the right
             if (newLeft.fastEquals(oldLeft) && canPruneRight(joinType) &&
-              filteringHasBenefit(right, left, r, hint)) {
+              filteringHasBenefit(right, left, r, l, hint)) {
               newRight = injectFilter(r, newRight, l, left)
             }
             if (!newLeft.fastEquals(oldLeft) || !newRight.fastEquals(oldRight)) {
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala
index 7640d9234c..6bf5db6d55 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala
@@ -169,6 +169,7 @@ trait LeafNode extends LogicalPlan with LeafLike[LogicalPlan] {
 
   /** Leaf nodes that can survive analysis must define their own statistics. */
   def computeStats(): Statistics = throw new UnsupportedOperationException
+
 }
 
 /**
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index 6df880bd4e..8812a341bd 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@ -341,6 +341,22 @@ object SQLConf {
       .booleanConf
       .createWithDefault(true)
 
+  val RUNTIME_FILTER_CREATION_SIDE_EXTEND_ENABLED =
+    buildConf("spark.sql.optimizer.runtimeFilter.creationSideExtend.enabled")
+      .doc("When true and if the plan is not a simple filter over scan, we attempt " +
+        "to get the origin likely selective plan according to exp.")
+      .version("3.3.0")
+      .booleanConf
+      .createWithDefault(false)
+
+  val RUNTIME_FILTER_ESTIMATE_STATS_ENABLED =
+    buildConf("spark.sql.optimizer.runtimeFilter.estimateStats.enabled")
+      .doc("When true and if sizeInBytes of the plan is unknown, we attempt " +
+        "to estimate the sizeInBytes.")
+      .version("3.3.0")
+      .booleanConf
+      .createWithDefault(false)
+
   val RUNTIME_FILTER_SEMI_JOIN_REDUCTION_ENABLED =
     buildConf("spark.sql.optimizer.runtimeFilter.semiJoinReduction.enabled")
       .doc("When true and if one side of a shuffle join has a selective predicate, we attempt " +
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala
index fd1824055d..e51d0bd874 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/HadoopFsRelation.scala
@@ -69,6 +69,11 @@ case class HadoopFsRelation(
     (location.sizeInBytes * compressionFactor).toLong
   }
 
+  override def estimateSizeInBytes: Long = {
+    val partitionDirectories: Seq[PartitionDirectory] = location.listFiles(Nil, Nil)
+    val files = partitionDirectories.flatMap(_.files)
+    if (files.isEmpty) 0 else files.map(_.getLen).sum
+  }
 
   override def inputFiles: Array[String] = location.inputFiles
 }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala
index 291b98fb37..d033bbc8a7 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/LogicalRelation.scala
@@ -22,6 +22,7 @@ import org.apache.spark.sql.catalyst.expressions.{AttributeMap, AttributeReferen
 import org.apache.spark.sql.catalyst.plans.QueryPlan
 import org.apache.spark.sql.catalyst.plans.logical.{ExposesMetadataColumns, LeafNode, LogicalPlan, Statistics}
 import org.apache.spark.sql.catalyst.util.{truncatedString, CharVarcharUtils}
+import org.apache.spark.sql.connector.read.SupportsEstimateStatistics
 import org.apache.spark.sql.sources.BaseRelation
 
 /**
@@ -32,7 +33,8 @@ case class LogicalRelation(
     output: Seq[AttributeReference],
     catalogTable: Option[CatalogTable],
     override val isStreaming: Boolean)
-  extends LeafNode with MultiInstanceRelation with ExposesMetadataColumns {
+  extends LeafNode with MultiInstanceRelation
+    with ExposesMetadataColumns with SupportsEstimateStatistics {
 
   // Only care about relation when canonicalizing.
   override def doCanonicalize(): LogicalPlan = copy(
@@ -45,6 +47,11 @@ case class LogicalRelation(
       .getOrElse(Statistics(sizeInBytes = relation.sizeInBytes))
   }
 
+  override def estimateStats(): Statistics = {
+    val sizeInBytes = relation.estimateSizeInBytes
+    Statistics(sizeInBytes = sizeInBytes)
+  }
+
   /** Used to lookup original attribute capitalization */
   val attributeMap: AttributeMap[AttributeReference] = AttributeMap(output.map(o => (o, o)))
 
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala
index d70c4b11bc..ca7b8b2c1f 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex.scala
@@ -110,7 +110,7 @@ abstract class PartitioningAwareFileIndex(
         throw new IllegalArgumentException(
           "Datasource with partition do not allow recursive file loading.")
       }
-      prunePartitions(partitionFilters, partitionSpec()).map {
+      prunePartitions(partitionFilters, partitionSpec()).toArray.map {
         case PartitionPath(values, path) =>
           val files: Seq[FileStatus] = leafDirToChildrenFiles.get(path) match {
             case Some(existingDir) =>
@@ -124,7 +124,7 @@ abstract class PartitioningAwareFileIndex(
           }
           PartitionDirectory(values, files)
       }
-    }
+    }.toSeq
     logTrace("Selected files after partition pruning:\n\t" + selectedPartitions.mkString("\n\t"))
     selectedPartitions
   }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala b/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
index 63e57c6804..5314669808 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
@@ -208,6 +208,8 @@ abstract class BaseRelation {
    */
   def sizeInBytes: Long = sqlContext.conf.defaultSizeInBytes
 
+  def estimateSizeInBytes: Long = sizeInBytes
+
   /**
    * Whether does it need to convert the objects in Row to internal representation, for example:
    *  java.lang.String to UTF8String
-- 
2.20.1

