From ab10501d03c49cba91847ee3b69c4e5275c87332 Mon Sep 17 00:00:00 2001
From: haojin <hao.jin@intel.com>
Date: Wed, 14 Sep 2022 12:25:27 +0800
Subject: [PATCH] Support to merge subquery plans with different filters

---
 .../optimizer/MergeScalarSubqueries.scala     | 389 -----------
 .../apache/spark/sql/internal/SQLConf.scala   |   9 +
 .../spark/sql/execution/SparkOptimizer.scala  |   6 +-
 .../datasources/FileSourceStrategy.scala      |  15 +-
 .../merge/MergeScalarSubqueries.scala         | 627 ++++++++++++++++++
 .../spark/sql/InjectRuntimeFilterSuite.scala  |   2 +-
 .../merge}/MergeScalarSubqueriesSuite.scala   |   0
 7 files changed, 652 insertions(+), 396 deletions(-)
 delete mode 100644 sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala
 create mode 100644 sql/core/src/main/scala/org/apache/spark/sql/execution/merge/MergeScalarSubqueries.scala
 rename sql/{catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer => core/src/test/scala/org/apache/spark/sql/execution/merge}/MergeScalarSubqueriesSuite.scala (100%)

diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala
deleted file mode 100644
index 44f3b653de..0000000000
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueries.scala
+++ /dev/null
@@ -1,389 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *    http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.spark.sql.catalyst.optimizer
-
-import scala.collection.mutable.ArrayBuffer
-
-import org.apache.spark.sql.catalyst.expressions._
-import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
-import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, CTERelationDef, CTERelationRef, Filter, Join, LogicalPlan, Project, Subquery, WithCTE}
-import org.apache.spark.sql.catalyst.rules.Rule
-import org.apache.spark.sql.catalyst.trees.TreePattern.{SCALAR_SUBQUERY, SCALAR_SUBQUERY_REFERENCE, TreePattern}
-import org.apache.spark.sql.internal.SQLConf
-import org.apache.spark.sql.types.DataType
-
-/**
- * This rule tries to merge multiple non-correlated [[ScalarSubquery]]s to compute multiple scalar
- * values once.
- *
- * The process is the following:
- * - While traversing through the plan each [[ScalarSubquery]] plan is tried to merge into the cache
- *   of already seen subquery plans. If merge is possible then cache is updated with the merged
- *   subquery plan, if not then the new subquery plan is added to the cache.
- *   During this first traversal each [[ScalarSubquery]] expression is replaced to a temporal
- *   [[ScalarSubqueryReference]] reference pointing to its cached version.
- *   The cache uses a flag to keep track of if a cache entry is a result of merging 2 or more
- *   plans, or it is a plan that was seen only once.
- *   Merged plans in the cache get a "Header", that contains the list of attributes form the scalar
- *   return value of a merged subquery.
- * - A second traversal checks if there are merged subqueries in the cache and builds a `WithCTE`
- *   node from these queries. The `CTERelationDef` nodes contain the merged subquery in the
- *   following form:
- *   `Project(Seq(CreateNamedStruct(name1, attribute1, ...) AS mergedValue), mergedSubqueryPlan)`
- *   and the definitions are flagged that they host a subquery, that can return maximum one row.
- *   During the second traversal [[ScalarSubqueryReference]] expressions that pont to a merged
- *   subquery is either transformed to a `GetStructField(ScalarSubquery(CTERelationRef(...)))`
- *   expression or restored to the original [[ScalarSubquery]].
- *
- * Eg. the following query:
- *
- * SELECT
- *   (SELECT avg(a) FROM t),
- *   (SELECT sum(b) FROM t)
- *
- * is optimized from:
- *
- * == Optimized Logical Plan ==
- * Project [scalar-subquery#242 [] AS scalarsubquery()#253,
- *          scalar-subquery#243 [] AS scalarsubquery()#254L]
- * :  :- Aggregate [avg(a#244) AS avg(a)#247]
- * :  :  +- Project [a#244]
- * :  :     +- Relation default.t[a#244,b#245] parquet
- * :  +- Aggregate [sum(a#251) AS sum(a)#250L]
- * :     +- Project [a#251]
- * :        +- Relation default.t[a#251,b#252] parquet
- * +- OneRowRelation
- *
- * to:
- *
- * == Optimized Logical Plan ==
- * Project [scalar-subquery#242 [].avg(a) AS scalarsubquery()#253,
- *          scalar-subquery#243 [].sum(a) AS scalarsubquery()#254L]
- * :  :- Project [named_struct(avg(a), avg(a)#247, sum(a), sum(a)#250L) AS mergedValue#260]
- * :  :  +- Aggregate [avg(a#244) AS avg(a)#247, sum(a#244) AS sum(a)#250L]
- * :  :     +- Project [a#244]
- * :  :        +- Relation default.t[a#244,b#245] parquet
- * :  +- Project [named_struct(avg(a), avg(a)#247, sum(a), sum(a)#250L) AS mergedValue#260]
- * :     +- Aggregate [avg(a#244) AS avg(a)#247, sum(a#244) AS sum(a)#250L]
- * :        +- Project [a#244]
- * :           +- Relation default.t[a#244,b#245] parquet
- * +- OneRowRelation
- *
- * == Physical Plan ==
- *  *(1) Project [Subquery scalar-subquery#242, [id=#125].avg(a) AS scalarsubquery()#253,
- *                ReusedSubquery
- *                  Subquery scalar-subquery#242, [id=#125].sum(a) AS scalarsubquery()#254L]
- * :  :- Subquery scalar-subquery#242, [id=#125]
- * :  :  +- *(2) Project [named_struct(avg(a), avg(a)#247, sum(a), sum(a)#250L) AS mergedValue#260]
- * :  :     +- *(2) HashAggregate(keys=[], functions=[avg(a#244), sum(a#244)],
- *                                output=[avg(a)#247, sum(a)#250L])
- * :  :        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#120]
- * :  :           +- *(1) HashAggregate(keys=[], functions=[partial_avg(a#244), partial_sum(a#244)],
- *                                      output=[sum#262, count#263L, sum#264L])
- * :  :              +- *(1) ColumnarToRow
- * :  :                 +- FileScan parquet default.t[a#244] ...
- * :  +- ReusedSubquery Subquery scalar-subquery#242, [id=#125]
- * +- *(1) Scan OneRowRelation[]
- */
-object MergeScalarSubqueries extends Rule[LogicalPlan] with PredicateHelper {
-  def apply(plan: LogicalPlan): LogicalPlan = {
-    plan match {
-      // Subquery reuse needs to be enabled for this optimization.
-      case _ if !conf.getConf(SQLConf.SUBQUERY_REUSE_ENABLED) => plan
-
-      // This rule does a whole plan traversal, no need to run on subqueries.
-      case _: Subquery => plan
-
-      // Plans with CTEs are not supported for now.
-      case _: WithCTE => plan
-
-      case _ => extractCommonScalarSubqueries(plan)
-    }
-  }
-
-  /**
-   * An item in the cache of merged scalar subqueries.
-   *
-   * @param attributes Attributes that form the struct scalar return value of a merged subquery.
-   * @param plan The plan of a merged scalar subquery.
-   * @param merged A flag to identify if this item is the result of merging subqueries.
-   *               Please note that `attributes.size == 1` doesn't always mean that the plan is not
-   *               merged as there can be subqueries that are different ([[checkIdenticalPlans]] is
-   *               false) due to an extra [[Project]] node in one of them. In that case
-   *               `attributes.size` remains 1 after merging, but the merged flag becomes true.
-   */
-  case class Header(attributes: Seq[Attribute], plan: LogicalPlan, merged: Boolean)
-
-  private def extractCommonScalarSubqueries(plan: LogicalPlan) = {
-    val cache = ArrayBuffer.empty[Header]
-    val planWithReferences = insertReferences(plan, cache)
-    cache.zipWithIndex.foreach { case (header, i) =>
-      cache(i) = cache(i).copy(plan =
-        if (header.merged) {
-          CTERelationDef(
-            createProject(header.attributes, removeReferences(header.plan, cache)),
-            underSubquery = true)
-        } else {
-          removeReferences(header.plan, cache)
-        })
-    }
-    val newPlan = removeReferences(planWithReferences, cache)
-    val subqueryCTEs = cache.filter(_.merged).map(_.plan.asInstanceOf[CTERelationDef])
-    if (subqueryCTEs.nonEmpty) {
-      WithCTE(newPlan, subqueryCTEs.toSeq)
-    } else {
-      newPlan
-    }
-  }
-
-  // First traversal builds up the cache and inserts `ScalarSubqueryReference`s to the plan.
-  private def insertReferences(plan: LogicalPlan, cache: ArrayBuffer[Header]): LogicalPlan = {
-    plan.transformUpWithSubqueries {
-      case n => n.transformExpressionsUpWithPruning(_.containsAnyPattern(SCALAR_SUBQUERY)) {
-        case s: ScalarSubquery if !s.isCorrelated && s.deterministic =>
-          val (subqueryIndex, headerIndex) = cacheSubquery(s.plan, cache)
-          ScalarSubqueryReference(subqueryIndex, headerIndex, s.dataType, s.exprId)
-      }
-    }
-  }
-
-  // Caching returns the index of the subquery in the cache and the index of scalar member in the
-  // "Header".
-  private def cacheSubquery(plan: LogicalPlan, cache: ArrayBuffer[Header]): (Int, Int) = {
-    val output = plan.output.head
-    cache.zipWithIndex.collectFirst(Function.unlift { case (header, subqueryIndex) =>
-      checkIdenticalPlans(plan, header.plan).map { outputMap =>
-        val mappedOutput = mapAttributes(output, outputMap)
-        val headerIndex = header.attributes.indexWhere(_.exprId == mappedOutput.exprId)
-        subqueryIndex -> headerIndex
-      }.orElse(tryMergePlans(plan, header.plan).map {
-        case (mergedPlan, outputMap) =>
-          val mappedOutput = mapAttributes(output, outputMap)
-          var headerIndex = header.attributes.indexWhere(_.exprId == mappedOutput.exprId)
-          val newHeaderAttributes = if (headerIndex == -1) {
-            headerIndex = header.attributes.size
-            header.attributes :+ mappedOutput
-          } else {
-            header.attributes
-          }
-          cache(subqueryIndex) = Header(newHeaderAttributes, mergedPlan, true)
-          subqueryIndex -> headerIndex
-      })
-    }).getOrElse {
-      cache += Header(Seq(output), plan, false)
-      cache.length - 1 -> 0
-    }
-  }
-
-  // If 2 plans are identical return the attribute mapping from the new to the cached version.
-  private def checkIdenticalPlans(
-      newPlan: LogicalPlan,
-      cachedPlan: LogicalPlan): Option[AttributeMap[Attribute]] = {
-    if (newPlan.canonicalized == cachedPlan.canonicalized) {
-      Some(AttributeMap(newPlan.output.zip(cachedPlan.output)))
-    } else {
-      None
-    }
-  }
-
-  // Recursively traverse down and try merging 2 plans. If merge is possible then return the merged
-  // plan with the attribute mapping from the new to the merged version.
-  // Please note that merging arbitrary plans can be complicated, the current version supports only
-  // some of the most important nodes.
-  private def tryMergePlans(
-      newPlan: LogicalPlan,
-      cachedPlan: LogicalPlan): Option[(LogicalPlan, AttributeMap[Attribute])] = {
-    checkIdenticalPlans(newPlan, cachedPlan).map(cachedPlan -> _).orElse(
-      (newPlan, cachedPlan) match {
-        case (np: Project, cp: Project) =>
-          tryMergePlans(np.child, cp.child).map { case (mergedChild, outputMap) =>
-            val (mergedProjectList, newOutputMap) =
-              mergeNamedExpressions(np.projectList, outputMap, cp.projectList)
-            val mergedPlan = Project(mergedProjectList, mergedChild)
-            mergedPlan -> newOutputMap
-          }
-        case (np, cp: Project) =>
-          tryMergePlans(np, cp.child).map { case (mergedChild, outputMap) =>
-            val (mergedProjectList, newOutputMap) =
-              mergeNamedExpressions(np.output, outputMap, cp.projectList)
-            val mergedPlan = Project(mergedProjectList, mergedChild)
-            mergedPlan -> newOutputMap
-          }
-        case (np: Project, cp) =>
-          tryMergePlans(np.child, cp).map { case (mergedChild, outputMap) =>
-            val (mergedProjectList, newOutputMap) =
-              mergeNamedExpressions(np.projectList, outputMap, cp.output)
-            val mergedPlan = Project(mergedProjectList, mergedChild)
-            mergedPlan -> newOutputMap
-          }
-        case (np: Aggregate, cp: Aggregate) if supportedAggregateMerge(np, cp) =>
-          tryMergePlans(np.child, cp.child).flatMap { case (mergedChild, outputMap) =>
-            val mappedNewGroupingExpression =
-              np.groupingExpressions.map(mapAttributes(_, outputMap))
-            // Order of grouping expression does matter as merging different grouping orders can
-            // introduce "extra" shuffles/sorts that might not present in all of the original
-            // subqueries.
-            if (mappedNewGroupingExpression.map(_.canonicalized) ==
-              cp.groupingExpressions.map(_.canonicalized)) {
-              val (mergedAggregateExpressions, newOutputMap) =
-                mergeNamedExpressions(np.aggregateExpressions, outputMap, cp.aggregateExpressions)
-              val mergedPlan =
-                Aggregate(cp.groupingExpressions, mergedAggregateExpressions, mergedChild)
-              Some(mergedPlan -> newOutputMap)
-            } else {
-              None
-            }
-          }
-
-        case (np: Filter, cp: Filter) =>
-          tryMergePlans(np.child, cp.child).flatMap { case (mergedChild, outputMap) =>
-            val mappedNewCondition = mapAttributes(np.condition, outputMap)
-            // Comparing the canonicalized form is required to ignore different forms of the same
-            // expression.
-            if (mappedNewCondition.canonicalized == cp.condition.canonicalized) {
-              val mergedPlan = cp.withNewChildren(Seq(mergedChild))
-              Some(mergedPlan -> outputMap)
-            } else {
-              None
-            }
-          }
-
-        case (np: Join, cp: Join) if np.joinType == cp.joinType && np.hint == cp.hint =>
-          tryMergePlans(np.left, cp.left).flatMap { case (mergedLeft, leftOutputMap) =>
-            tryMergePlans(np.right, cp.right).flatMap { case (mergedRight, rightOutputMap) =>
-              val outputMap = leftOutputMap ++ rightOutputMap
-              val mappedNewCondition = np.condition.map(mapAttributes(_, outputMap))
-              // Comparing the canonicalized form is required to ignore different forms of the same
-              // expression and `AttributeReference.quailifier`s in `cp.condition`.
-              if (mappedNewCondition.map(_.canonicalized) == cp.condition.map(_.canonicalized)) {
-                val mergedPlan = cp.withNewChildren(Seq(mergedLeft, mergedRight))
-                Some(mergedPlan -> outputMap)
-              } else {
-                None
-              }
-            }
-          }
-
-        // Otherwise merging is not possible.
-        case _ => None
-      })
-  }
-
-  private def createProject(attributes: Seq[Attribute], plan: LogicalPlan): Project = {
-    Project(
-      Seq(Alias(
-        CreateNamedStruct(attributes.flatMap(a => Seq(Literal(a.name), a))),
-        "mergedValue")()),
-      plan)
-  }
-
-  private def mapAttributes[T <: Expression](expr: T, outputMap: AttributeMap[Attribute]) = {
-    expr.transform {
-      case a: Attribute => outputMap.getOrElse(a, a)
-    }.asInstanceOf[T]
-  }
-
-  // Applies `outputMap` attribute mapping on attributes of `newExpressions` and merges them into
-  // `cachedExpressions`. Returns the merged expressions and the attribute mapping from the new to
-  // the merged version that can be propagated up during merging nodes.
-  private def mergeNamedExpressions(
-      newExpressions: Seq[NamedExpression],
-      outputMap: AttributeMap[Attribute],
-      cachedExpressions: Seq[NamedExpression]) = {
-    val mergedExpressions = ArrayBuffer[NamedExpression](cachedExpressions: _*)
-    val newOutputMap = AttributeMap(newExpressions.map { ne =>
-      val mapped = mapAttributes(ne, outputMap)
-      val withoutAlias = mapped match {
-        case Alias(child, _) => child
-        case e => e
-      }
-      ne.toAttribute -> mergedExpressions.find {
-        case Alias(child, _) => child semanticEquals withoutAlias
-        case e => e semanticEquals withoutAlias
-      }.getOrElse {
-        mergedExpressions += mapped
-        mapped
-      }.toAttribute
-    })
-    (mergedExpressions.toSeq, newOutputMap)
-  }
-
-  // Only allow aggregates of the same implementation because merging different implementations
-  // could cause performance regression.
-  private def supportedAggregateMerge(newPlan: Aggregate, cachedPlan: Aggregate) = {
-    val newPlanAggregateExpressions = newPlan.aggregateExpressions.flatMap(_.collect {
-      case a: AggregateExpression => a
-    })
-    val cachedPlanAggregateExpressions = cachedPlan.aggregateExpressions.flatMap(_.collect {
-      case a: AggregateExpression => a
-    })
-    val newPlanSupportsHashAggregate = Aggregate.supportsHashAggregate(
-      newPlanAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes))
-    val cachedPlanSupportsHashAggregate = Aggregate.supportsHashAggregate(
-      cachedPlanAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes))
-    newPlanSupportsHashAggregate && cachedPlanSupportsHashAggregate ||
-      newPlanSupportsHashAggregate == cachedPlanSupportsHashAggregate && {
-        val newPlanSupportsObjectHashAggregate =
-          Aggregate.supportsObjectHashAggregate(newPlanAggregateExpressions)
-        val cachedPlanSupportsObjectHashAggregate =
-          Aggregate.supportsObjectHashAggregate(cachedPlanAggregateExpressions)
-        newPlanSupportsObjectHashAggregate && cachedPlanSupportsObjectHashAggregate ||
-          newPlanSupportsObjectHashAggregate == cachedPlanSupportsObjectHashAggregate
-      }
-  }
-
-  // Second traversal replaces `ScalarSubqueryReference`s to either
-  // `GetStructField(ScalarSubquery(CTERelationRef to the merged plan)` if the plan is merged from
-  // multiple subqueries or `ScalarSubquery(original plan)` if it isn't.
-  private def removeReferences(
-      plan: LogicalPlan,
-      cache: ArrayBuffer[Header]) = {
-    plan.transformUpWithSubqueries {
-      case n =>
-        n.transformExpressionsWithPruning(_.containsAnyPattern(SCALAR_SUBQUERY_REFERENCE)) {
-          case ssr: ScalarSubqueryReference =>
-            val header = cache(ssr.subqueryIndex)
-            if (header.merged) {
-              val subqueryCTE = header.plan.asInstanceOf[CTERelationDef]
-              GetStructField(
-                ScalarSubquery(
-                  CTERelationRef(subqueryCTE.id, _resolved = true, subqueryCTE.output),
-                  exprId = ssr.exprId),
-                ssr.headerIndex)
-            } else {
-              ScalarSubquery(header.plan, exprId = ssr.exprId)
-            }
-        }
-    }
-  }
-}
-
-/**
- * Temporal reference to a subquery.
- */
-case class ScalarSubqueryReference(
-    subqueryIndex: Int,
-    headerIndex: Int,
-    dataType: DataType,
-    exprId: ExprId) extends LeafExpression with Unevaluable {
-  override def nullable: Boolean = true
-
-  final override val nodePatterns: Seq[TreePattern] = Seq(SCALAR_SUBQUERY_REFERENCE)
-
-  override def stringArgs: Iterator[Any] = Iterator(subqueryIndex, headerIndex, dataType, exprId.id)
-}
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
index 8812a341bd..c81ae1f487 100644
--- a/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala
@@ -3771,6 +3771,15 @@ object SQLConf {
       .booleanConf
       .createWithDefault(false)
 
+  val PLAN_MERGE_IGNORE_PUSHED_PUSHED_DATA_FILTERS =
+    buildConf("spark.sql.planMerge.ignorePushedDataFilters")
+      .internal()
+      .doc(s"When set to true plan merging is enabled even if physical scan operations have " +
+        "different data filters pushed down.")
+      .version("3.3.0")
+      .booleanConf
+      .createWithDefault(true)
+
   /**
    * Holds information about keys that have been deprecated.
    *
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala
index 84e5975189..c5c66af797 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkOptimizer.scala
@@ -27,6 +27,7 @@ import org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions
 import org.apache.spark.sql.execution.datasources.SchemaPruning
 import org.apache.spark.sql.execution.datasources.v2.{GroupBasedRowLevelOperationScanPlanning, OptimizeMetadataOnlyDeleteFromTable, V2ScanPartitioning, V2ScanRelationPushDown, V2Writes}
 import org.apache.spark.sql.execution.dynamicpruning.{CleanupDynamicPruningFilters, PartitionPruning}
+import org.apache.spark.sql.execution.merge.MergeScalarSubqueries
 import org.apache.spark.sql.execution.python.{ExtractGroupingPythonUDFFromAggregate, ExtractPythonUDFFromAggregate, ExtractPythonUDFs}
 
 class SparkOptimizer(
@@ -54,8 +55,6 @@ class SparkOptimizer(
     Batch("InjectRuntimeFilter", FixedPoint(1),
       InjectRuntimeFilter,
       RewritePredicateSubquery) :+
-    Batch("MergeScalarSubqueries", Once,
-      MergeScalarSubqueries) :+
     Batch("Pushdown Filters from PartitionPruning", fixedPoint,
       PushDownPredicates) :+
     Batch("Cleanup filters that cannot be pushed down", Once,
@@ -79,6 +78,9 @@ class SparkOptimizer(
       PushPredicateThroughNonJoin,
       RemoveNoopOperators) :+
     Batch("User Provided Optimizers", fixedPoint, experimentalMethods.extraOptimizations: _*) :+
+    Batch("Merge Scalar Subqueries", Once,
+      MergeScalarSubqueries,
+      RewriteDistinctAggregates) :+
     Batch("Replace CTE with Repartition", Once, ReplaceCTERefWithRepartition)
 
   override def nonExcludableRules: Seq[String] = super.nonExcludableRules :+
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala
index 9356e46a69..6a47e95622 100644
--- a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileSourceStrategy.scala
@@ -52,8 +52,15 @@ import org.apache.spark.util.collection.BitSet
  *     is under the threshold with the addition of the next file, add it.  If not, open a new bucket
  *     and add it.  Proceed to the next file.
  */
-object FileSourceStrategy extends Strategy with PredicateHelper with Logging {
+object FileSourceStrategy extends Strategy {
 
+  def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {
+    case FileSourceScanPlan(scanPlan, _) => scanPlan :: Nil
+    case _ => Nil
+  }
+}
+
+object FileSourceScanPlan extends PredicateHelper with Logging {
   // should prune buckets iff num buckets is greater than 1 and there is only one bucket column
   private def shouldPruneBuckets(bucketSpec: Option[BucketSpec]): Boolean = {
     bucketSpec match {
@@ -143,7 +150,7 @@ object FileSourceStrategy extends Strategy with PredicateHelper with Logging {
     }
   }
 
-  def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {
+  def unapply(plan: LogicalPlan): Option[(SparkPlan, FileSourceScanExec)] = plan match {
     case ScanOperation(projects, filters,
       l @ LogicalRelation(fsRelation: HadoopFsRelation, _, table, _)) =>
       // Filters on this relation fall into four categories based on where we can use them to avoid
@@ -255,8 +262,8 @@ object FileSourceStrategy extends Strategy with PredicateHelper with Logging {
         execution.ProjectExec(projects, withFilter)
       }
 
-      withProjections :: Nil
+      Some(withProjections, scan)
 
-    case _ => Nil
+    case _ => None
   }
 }
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/merge/MergeScalarSubqueries.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/merge/MergeScalarSubqueries.scala
new file mode 100644
index 0000000000..0d259a48de
--- /dev/null
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/merge/MergeScalarSubqueries.scala
@@ -0,0 +1,627 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.spark.sql.execution.merge
+
+import scala.collection.mutable.ArrayBuffer
+
+import org.apache.spark.sql.catalyst.expressions._
+import org.apache.spark.sql.catalyst.expressions.aggregate.{AggregateExpression, Average, Count, Max, Min, Sum}
+import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, CTERelationDef, CTERelationRef, Filter, Join, LocalRelation, LogicalPlan, Project, SerializeFromObject, Subquery, WithCTE}
+import org.apache.spark.sql.catalyst.rules.Rule
+import org.apache.spark.sql.catalyst.trees.TreePattern.{SCALAR_SUBQUERY, SCALAR_SUBQUERY_REFERENCE, TreePattern}
+import org.apache.spark.sql.execution.ExternalRDD
+import org.apache.spark.sql.execution.datasources.FileSourceScanPlan
+import org.apache.spark.sql.internal.SQLConf
+import org.apache.spark.sql.types.DataType
+
+/**
+ * This rule tries to merge multiple non-correlated [[ScalarSubquery]]s to compute multiple scalar
+ * values once.
+ *
+ * The process is the following:
+ * - While traversing through the plan each [[ScalarSubquery]] plan is tried to merge into the cache
+ *   of already seen subquery plans. If merge is possible then cache is updated with the merged
+ *   subquery plan, if not then the new subquery plan is added to the cache.
+ *   During this first traversal each [[ScalarSubquery]] expression is replaced to a temporal
+ *   [[ScalarSubqueryReference]] reference pointing to its cached version.
+ *   The cache uses a flag to keep track of if a cache entry is a result of merging 2 or more
+ *   plans, or it is a plan that was seen only once.
+ *   Merged plans in the cache get a "Header", that contains the list of attributes form the scalar
+ *   return value of a merged subquery.
+ * - A second traversal checks if there are merged subqueries in the cache and builds a `WithCTE`
+ *   node from these queries. The `CTERelationDef` nodes contain the merged subquery in the
+ *   following form:
+ *   `Project(Seq(CreateNamedStruct(name1, attribute1, ...) AS mergedValue), mergedSubqueryPlan)`
+ *   and the definitions are flagged that they host a subquery, that can return maximum one row.
+ *   During the second traversal [[ScalarSubqueryReference]] expressions that pont to a merged
+ *   subquery is either transformed to a `GetStructField(ScalarSubquery(CTERelationRef(...)))`
+ *   expression or restored to the original [[ScalarSubquery]].
+ *
+ * Eg. the following query:
+ *
+ * SELECT
+ *   (SELECT avg(a) FROM t),
+ *   (SELECT sum(b) FROM t)
+ *
+ * is optimized from:
+ *
+ * == Optimized Logical Plan ==
+ * Project [scalar-subquery#242 [] AS scalarsubquery()#253,
+ *          scalar-subquery#243 [] AS scalarsubquery()#254L]
+ * :  :- Aggregate [avg(a#244) AS avg(a)#247]
+ * :  :  +- Project [a#244]
+ * :  :     +- Relation default.t[a#244,b#245] parquet
+ * :  +- Aggregate [sum(a#251) AS sum(a)#250L]
+ * :     +- Project [a#251]
+ * :        +- Relation default.t[a#251,b#252] parquet
+ * +- OneRowRelation
+ *
+ * to:
+ *
+ * == Optimized Logical Plan ==
+ * Project [scalar-subquery#242 [].avg(a) AS scalarsubquery()#253,
+ *          scalar-subquery#243 [].sum(a) AS scalarsubquery()#254L]
+ * :  :- Project [named_struct(avg(a), avg(a)#247, sum(a), sum(a)#250L) AS mergedValue#260]
+ * :  :  +- Aggregate [avg(a#244) AS avg(a)#247, sum(a#244) AS sum(a)#250L]
+ * :  :     +- Project [a#244]
+ * :  :        +- Relation default.t[a#244,b#245] parquet
+ * :  +- Project [named_struct(avg(a), avg(a)#247, sum(a), sum(a)#250L) AS mergedValue#260]
+ * :     +- Aggregate [avg(a#244) AS avg(a)#247, sum(a#244) AS sum(a)#250L]
+ * :        +- Project [a#244]
+ * :           +- Relation default.t[a#244,b#245] parquet
+ * +- OneRowRelation
+ *
+ * == Physical Plan ==
+ *  *(1) Project [Subquery scalar-subquery#242, [id=#125].avg(a) AS scalarsubquery()#253,
+ *                ReusedSubquery
+ *                  Subquery scalar-subquery#242, [id=#125].sum(a) AS scalarsubquery()#254L]
+ * :  :- Subquery scalar-subquery#242, [id=#125]
+ * :  :  +- *(2) Project [named_struct(avg(a), avg(a)#247, sum(a), sum(a)#250L) AS mergedValue#260]
+ * :  :     +- *(2) HashAggregate(keys=[], functions=[avg(a#244), sum(a#244)],
+ *                                output=[avg(a)#247, sum(a)#250L])
+ * :  :        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#120]
+ * :  :           +- *(1) HashAggregate(keys=[], functions=[partial_avg(a#244), partial_sum(a#244)],
+ *                                      output=[sum#262, count#263L, sum#264L])
+ * :  :              +- *(1) ColumnarToRow
+ * :  :                 +- FileScan parquet default.t[a#244] ...
+ * :  +- ReusedSubquery Subquery scalar-subquery#242, [id=#125]
+ * +- *(1) Scan OneRowRelation[]
+ */
+object MergeScalarSubqueries extends Rule[LogicalPlan] {
+  def apply(plan: LogicalPlan): LogicalPlan = {
+    plan match {
+      // Subquery reuse needs to be enabled for this optimization.
+      case _ if !conf.getConf(SQLConf.SUBQUERY_REUSE_ENABLED) => plan
+
+      // This rule does a whole plan traversal, no need to run on subqueries.
+      case _: Subquery => plan
+
+      // Plans with CTEs are not supported for now.
+      case _: WithCTE => plan
+
+      case _ => extractCommonScalarSubqueries(plan)
+    }
+  }
+
+  /**
+   * An item in the cache of merged scalar subqueries.
+   *
+   * @param attributes Attributes that form the struct scalar return value of a merged subquery.
+   * @param plan The plan of a merged scalar subquery.
+   * @param merged A flag to identify if this item is the result of merging subqueries.
+   *               Please note that `attributes.size == 1` doesn't always mean that the plan is not
+   *               merged as there can be subqueries that are different ([[checkIdenticalPlans]] is
+   *               false) due to an extra [[Project]] node in one of them. In that case
+   *               `attributes.size` remains 1 after merging, but the merged flag becomes true.
+   */
+  case class Header(attributes: Seq[Attribute], plan: LogicalPlan, merged: Boolean)
+
+  private def extractCommonScalarSubqueries(plan: LogicalPlan) = {
+    val cache = ArrayBuffer.empty[Header]
+    val planWithReferences = insertReferences(plan, cache)
+    cache.zipWithIndex.foreach { case (header, i) =>
+      cache(i) = cache(i).copy(plan =
+        if (header.merged) {
+          CTERelationDef(
+            createProject(header.attributes,
+              removePropagatedFilters(removeReferences(header.plan, cache))),
+            underSubquery = true)
+        } else {
+          removePropagatedFilters(removeReferences(header.plan, cache))
+        })
+    }
+    val newPlan = removePropagatedFilters(removeReferences(planWithReferences, cache))
+    val subqueryCTEs = cache.filter(_.merged).map(_.plan.asInstanceOf[CTERelationDef])
+    if (subqueryCTEs.nonEmpty) {
+      WithCTE(newPlan, subqueryCTEs.toSeq)
+    } else {
+      newPlan
+    }
+  }
+
+  // First traversal builds up the cache and inserts `ScalarSubqueryReference`s to the plan.
+  private def insertReferences(plan: LogicalPlan, cache: ArrayBuffer[Header]): LogicalPlan = {
+    plan.transformUpWithSubqueries {
+      case n => n.transformExpressionsUpWithPruning(_.containsAnyPattern(SCALAR_SUBQUERY)) {
+        case s: ScalarSubquery if !s.isCorrelated && s.deterministic =>
+          val (subqueryIndex, headerIndex) = cacheSubquery(s.plan, cache)
+          ScalarSubqueryReference(subqueryIndex, headerIndex, s.dataType, s.exprId)
+      }
+    }
+  }
+
+  // State of the plan merging algorithm
+  object ScanCheck extends Enumeration {
+    type ScanCheck = Value
+
+    // There is no need to check if physical plan is mergeable until we don't encounter `Filter`s
+    // with different predicates
+    val NO_NEED,
+
+    // We switch to this state once we encounter different `Filters` in the plans we want to merge.
+    // Identical logical plans is not enough to consider the plans mergeagle, but we also need to
+    // check physical scans to determine if partition, bucket and other pushed down filters are
+    // safely mergeable without performance degradation.
+    CHECKING,
+
+    DONE = Value
+    // Once the physical check is complete we use this state to finish the logical merge check.
+  }
+
+  import ScanCheck._
+
+  // Caching returns the index of the subquery in the cache and the index of scalar member in the
+  // "Header".
+  private def cacheSubquery(plan: LogicalPlan, cache: ArrayBuffer[Header]): (Int, Int) = {
+    val output = plan.output.head
+    cache.zipWithIndex.collectFirst(Function.unlift { case (header, subqueryIndex) =>
+      checkIdenticalPlans(plan, header.plan).map { outputMap =>
+        val mappedOutput = mapAttributes(output, outputMap)
+        val headerIndex = header.attributes.indexWhere(_.exprId == mappedOutput.exprId)
+        subqueryIndex -> headerIndex
+      }.orElse(tryMergePlans(plan, header.plan, NO_NEED).collect {
+        case (mergedPlan, outputMap, None, None) =>
+          val mappedOutput = mapAttributes(output, outputMap)
+          var headerIndex = header.attributes.indexWhere(_.exprId == mappedOutput.exprId)
+          val newHeaderAttributes = if (headerIndex == -1) {
+            headerIndex = header.attributes.size
+            header.attributes :+ mappedOutput
+          } else {
+            header.attributes
+          }
+          cache(subqueryIndex) = Header(newHeaderAttributes, mergedPlan, true)
+          subqueryIndex -> headerIndex
+      })
+    }).getOrElse {
+      cache += Header(Seq(output), plan, false)
+      cache.length - 1 -> 0
+    }
+  }
+
+  // If 2 plans are identical return the attribute mapping from the new to the cached version.
+  private def checkIdenticalPlans(
+                                   newPlan: LogicalPlan,
+                                   cachedPlan: LogicalPlan): Option[AttributeMap[Attribute]] = {
+    if (newPlan.canonicalized == cachedPlan.canonicalized) {
+      Some(AttributeMap(newPlan.output.zip(cachedPlan.output)))
+    } else {
+      None
+    }
+  }
+
+  // Recursively traverse down and try merging 2 plans. If merge is possible then returns:
+  // - the merged plan,
+  // - the attribute mapping from the new to the merged version,
+  // - optional filters of both plans that need to be propagated and merged in an ancestor
+  // `Aggregate` node if possible.
+  //
+  // Please note that merging arbitrary plans can be complicated, the current version supports only
+  // some of the most important nodes.
+  private def tryMergePlans(
+                             newPlan: LogicalPlan,
+                             cachedPlan: LogicalPlan,
+                             scanCheck: ScanCheck):
+  Option[(LogicalPlan, AttributeMap[Attribute], Option[Expression], Option[Expression])] = {
+    (if (scanCheck == CHECKING) {
+      // If physical check is needed then matching logical plans is not enough.
+      None
+    } else {
+      checkIdenticalPlans(newPlan, cachedPlan).map((cachedPlan, _, None, None))
+    }).orElse(
+      (scanCheck, newPlan, cachedPlan) match {
+        case (CHECKING, SerializeFromObject(_, _: ExternalRDD[_]),
+        SerializeFromObject(_, _: ExternalRDD[_])) =>
+          checkIdenticalPlans(newPlan, cachedPlan).map((cachedPlan, _, None, None))
+
+        case (CHECKING, _: LocalRelation, _: LocalRelation) =>
+          checkIdenticalPlans(newPlan, cachedPlan).map((cachedPlan, _, None, None))
+
+        case (CHECKING, FileSourceScanPlan(_, newScan), FileSourceScanPlan(_, cachedScan)) =>
+          val (newScanToCompare, cachedScanToCompare) =
+            if (conf.getConf(SQLConf.PLAN_MERGE_IGNORE_PUSHED_PUSHED_DATA_FILTERS)) {
+              (newScan.copy(dataFilters = Seq.empty), cachedScan.copy(dataFilters = Seq.empty))
+            } else {
+              (newScan, cachedScan)
+            }
+          if (newScanToCompare.canonicalized == cachedScanToCompare.canonicalized) {
+            // Physical plan is mergeable, but we still need to finish the logical merge to
+            // propagate the filters
+            tryMergePlans(newPlan, cachedPlan, DONE)
+          } else {
+            None
+          }
+
+        case (_, np: Project, cp: Project) =>
+          tryMergePlans(np.child, cp.child, scanCheck).map {
+            case (mergedChild, outputMap, newChildFilter, mergedChildFilter) =>
+              val (mergedProjectList, newOutputMap, newPlanFilter, mergedPlanFilter) =
+                mergeNamedExpressions(np.projectList, outputMap, cp.projectList, newChildFilter,
+                  mergedChildFilter)
+              val mergedPlan = Project(mergedProjectList, mergedChild)
+              (mergedPlan, newOutputMap, newPlanFilter, mergedPlanFilter)
+          }
+        case (_, np, cp: Project) =>
+          tryMergePlans(np, cp.child, scanCheck).map {
+            case (mergedChild, outputMap, newChildFilter, mergedChildFilter) =>
+              val (mergedProjectList, newOutputMap, newPlanFilter, mergedPlanFilter) =
+                mergeNamedExpressions(np.output, outputMap, cp.projectList, newChildFilter,
+                  mergedChildFilter)
+              val mergedPlan = Project(mergedProjectList, mergedChild)
+              (mergedPlan, newOutputMap, newPlanFilter, mergedPlanFilter)
+          }
+        case (_, np: Project, cp) =>
+          tryMergePlans(np.child, cp, scanCheck).map {
+            case (mergedChild, outputMap, newChildFilter, mergedChildFilter) =>
+              val (mergedProjectList, newOutputMap, newPlanFilter, mergedPlanFilter) =
+                mergeNamedExpressions(np.projectList, outputMap, cp.output, newChildFilter,
+                  mergedChildFilter)
+              val mergedPlan = Project(mergedProjectList, mergedChild)
+              (mergedPlan, newOutputMap, newPlanFilter, mergedPlanFilter)
+          }
+        case (_, np: Aggregate, cp: Aggregate) if supportedAggregateMerge(np, cp) =>
+          tryMergePlans(np.child, cp.child, scanCheck).flatMap {
+            case (mergedChild, outputMap, None, None) =>
+              val mappedNewGroupingExpression =
+                np.groupingExpressions.map(mapAttributes(_, outputMap))
+              // Order of grouping expression does matter as merging different grouping orders can
+              // introduce "extra" shuffles/sorts that might not present in all of the original
+              // subqueries.
+              if (mappedNewGroupingExpression.map(_.canonicalized) ==
+                cp.groupingExpressions.map(_.canonicalized)) {
+                val (mergedAggregateExpressions, newOutputMap, _, _) =
+                  mergeNamedExpressions(np.aggregateExpressions, outputMap, cp.aggregateExpressions,
+                    None, None)
+                val mergedPlan =
+                  Aggregate(cp.groupingExpressions, mergedAggregateExpressions, mergedChild)
+                Some(mergedPlan, newOutputMap, None, None)
+              } else {
+                None
+              }
+            case (mergedChild, outputMap, newChildFilter, mergedChildFilter)
+              if supportsFilterPropagation(np) && supportsFilterPropagation(cp) =>
+              val (mergedAggregateExpressions, newOutputMap, _, _) =
+                mergeNamedExpressions(
+                  filterAggregateExpressions(np.aggregateExpressions, newChildFilter),
+                  outputMap, filterAggregateExpressions(cp.aggregateExpressions, mergedChildFilter),
+                  None, None)
+              val mergedPlan =
+                Aggregate(cp.groupingExpressions, mergedAggregateExpressions, mergedChild)
+              Some(mergedPlan, newOutputMap, None, None)
+            case _ => None
+          }
+
+        // If `Filter`s are not exactly the same we can still try propagating up their differing
+        // condition because in some cases we will be able to merge them in an `Aggregate` parent
+        // node.
+        // E.g.:
+        //   SELECT avg(a) FROM t WHERE c = 1
+        // and:
+        //   SELECT sum(b) FROM t WHERE c = 2
+        // can be merged to:
+        // SELECT namedStruct(
+        //   'a', avg(a) FILTER (WHERE c = 1),
+        //   'b', sum(b) FILTER (WHERE c = 2)) AS mergedValue
+        // FORM t
+        // WHERE c = 1 OR c = 2
+        //
+        // Please note that depending on where the different `Filter`s reside in the plan and on
+        // which column the predicates are defined, we need to check the physical plan to make sure
+        // if `c` is not a partitioning or bucketing column and `c` is not present in pushed down
+        // filters. Otherwise the merged query can suffer performance degradation.
+        case (_, np: Filter, cp: Filter) =>
+          tryMergePlans(np.child, cp.child, scanCheck).flatMap {
+            case (mergedChild, outputMap, newChildFilter, mergedChildFilter) =>
+              val mappedNewCondition = mapAttributes(np.condition, outputMap)
+              // Comparing the canonicalized form is required to ignore different forms of the same
+              // expression.
+              if (mappedNewCondition.canonicalized == cp.condition.canonicalized) {
+                val filters = (mergedChildFilter.toSeq ++ newChildFilter.toSeq).reduceOption(Or)
+                  .map(PropagatedFilter)
+                // Please note that we construct the merged `Filter` condition in a way that the
+                // filters we propagate are on the right side of the `And` condition so as to be
+                // able to extract the already propagated filters in `extractNonPropagatedFilter()`
+                // easily.
+                val mergedCondition = (cp.condition +: filters.toSeq).reduce(And)
+                val mergedPlan = Filter(mergedCondition, mergedChild)
+                Some(mergedPlan, outputMap, newChildFilter, mergedChildFilter)
+              } else if (scanCheck == NO_NEED) {
+                tryMergePlans(np, cp, CHECKING)
+              } else {
+                val newPlanFilter = (mappedNewCondition +: newChildFilter.toSeq).reduce(And)
+                val cachedPlanFilter = (cp.condition +: mergedChildFilter.toSeq).reduce(And)
+                val mergedCondition = PropagatedFilter(Or(cachedPlanFilter, newPlanFilter))
+                val mergedPlan = Filter(mergedCondition, mergedChild)
+                // There might be `PropagatedFilter`s in the cached plan's `Filter` that we don't
+                // need to re-propagate.
+                val nonPropagatedCachedFilter = extractNonPropagatedFilter(cp.condition)
+                val mergedPlanFilter =
+                  (mergedChildFilter.toSeq ++ nonPropagatedCachedFilter.toSeq).reduceOption(And)
+                Some(mergedPlan, outputMap, Some(newPlanFilter), mergedPlanFilter)
+              }
+          }
+        case (NO_NEED, np, cp: Filter) => tryMergePlans(np, cp, CHECKING)
+        case (_, np, cp: Filter) =>
+          tryMergePlans(np, cp.child, scanCheck).map {
+            case (mergedChild, outputMap, newChildFilter, mergedChildFilter) =>
+              // There might be `PropagatedFilter`s in the cached plan's `Filter` and we don't
+              // need to re-propagate them.
+              val nonPropagatedCachedFilter = extractNonPropagatedFilter(cp.condition)
+              val mergedPlanFilter =
+                (mergedChildFilter.toSeq ++ nonPropagatedCachedFilter.toSeq).reduceOption(And)
+              (mergedChild, outputMap, newChildFilter, mergedPlanFilter)
+          }
+        case (NO_NEED, np: Filter, cp) => tryMergePlans(np, cp, CHECKING)
+        case (_, np: Filter, cp) =>
+          tryMergePlans(np.child, cp, scanCheck).map {
+            case (mergedChild, outputMap, newChildFilter, mergedChildFilter) =>
+              val mappedNewCondition = mapAttributes(np.condition, outputMap)
+              val planFilter = (mappedNewCondition +: newChildFilter.toSeq).reduce(And)
+              (mergedChild, outputMap, Some(planFilter), mergedChildFilter)
+          }
+
+        case (_, np: Join, cp: Join) if np.joinType == cp.joinType && np.hint == cp.hint =>
+          tryMergePlans(np.left, cp.left, scanCheck).flatMap {
+            case (mergedLeft, leftOutputMap, leftNewChildFilter, leftMergedChildFilter) =>
+              tryMergePlans(np.right, cp.right, scanCheck).flatMap {
+                case (mergedRight, rightOutputMap, rightNewChildFilter, rightMergedChildFilter) =>
+                  val outputMap = leftOutputMap ++ rightOutputMap
+                  val mappedNewCondition = np.condition.map(mapAttributes(_, outputMap))
+                  // Comparing the canonicalized form is required to ignore different forms of the
+                  // same expression and `AttributeReference.quailifier`s in `cp.condition`.
+                  if (mappedNewCondition.map(_.canonicalized) ==
+                    cp.condition.map(_.canonicalized)) {
+                    val mergedPlan = cp.withNewChildren(Seq(mergedLeft, mergedRight))
+                    val planFilter =
+                      (leftNewChildFilter.toSeq ++ rightNewChildFilter.toSeq).reduceOption(And)
+                    val mergedPlanFilter = (leftMergedChildFilter.toSeq ++
+                      rightMergedChildFilter.toSeq).reduceOption(And)
+                    Some(mergedPlan, outputMap, planFilter, mergedPlanFilter)
+                  } else {
+                    None
+                  }
+              }
+          }
+
+        // Otherwise merging is not possible.
+        case _ => None
+      }
+    )
+  }
+
+  private def createProject(attributes: Seq[Attribute], plan: LogicalPlan): Project = {
+    Project(
+      Seq(Alias(
+        CreateNamedStruct(attributes.flatMap(a => Seq(Literal(a.name), a))),
+        "mergedValue")()),
+      plan)
+  }
+
+  private def mapAttributes[T <: Expression](expr: T, outputMap: AttributeMap[Attribute]) = {
+    expr.transform {
+      case a: Attribute => outputMap.getOrElse(a, a)
+    }.asInstanceOf[T]
+  }
+
+  /**
+   * - When we merge projection nodes (`Project` and `Aggregate`) we need to merge the named
+   * expression list coming from the new plan node into the expressions of the projection node of
+   * the merged child plan and return a merged list of expressions that will be placed into the
+   * merged projection node.
+   * - Before we can merge the new expressions, we need to take into account the propagated
+   * attribute mapping that describes the transformation from the input attributes the new plan's
+   * projection node to the input attributes of the merged child plan's projection node.
+   * - While merging the new expressions we need to build a new attribute mapping that describes
+   * the transformation from the output attributes of the new expressions to the output attributes
+   * of the merged list of expression.
+   * - If any filters are propagated from `Filter` nodes below, we need to transform the expressions
+   * to named expressions and merge them into the cached expressions as we did with new expressions.
+   *
+   * @param newExpressions the expressions of the new plan's projection node
+   * @param outputMap the propagated attribute mapping
+   * @param cachedExpressions the expressions of the cached plan's projection node
+   * @param newChildFilter the propagated filters from `Filter` nodes of the new plan
+   * @param mergedChildFilter the propagated filters from `Filter` nodes of the merged child plan
+   * @return A tuple of:
+   *         - the merged expression list,
+   *         - the new attribute mapping to propagate,
+   *         - the output attributes of the merged newChildFilter to propagate,
+   *         - the output attributes of the merged mergedChildFilter to propagate,
+   */
+  private def mergeNamedExpressions(
+                                     newExpressions: Seq[NamedExpression],
+                                     outputMap: AttributeMap[Attribute],
+                                     cachedExpressions: Seq[NamedExpression],
+                                     newChildFilter: Option[Expression],
+                                     mergedChildFilter: Option[Expression]):
+  (Seq[NamedExpression], AttributeMap[Attribute], Option[Attribute], Option[Attribute]) = {
+    val mergedExpressions = ArrayBuffer[NamedExpression](cachedExpressions: _*)
+    val newOutputMap = AttributeMap(newExpressions.map { ne =>
+      val mapped = mapAttributes(ne, outputMap)
+      val withoutAlias = mapped match {
+        case Alias(child, _) => child
+        case e => e
+      }
+      ne.toAttribute -> mergedExpressions.find {
+        case Alias(child, _) => child semanticEquals withoutAlias
+        case e => e semanticEquals withoutAlias
+      }.getOrElse {
+        mergedExpressions += mapped
+        mapped
+      }.toAttribute
+    })
+
+    def mergeFilter(filter: Option[Expression]) = {
+      filter.map { f =>
+        mergedExpressions.find {
+          case Alias(child, _) => child semanticEquals f
+          case e => e semanticEquals f
+        }.getOrElse {
+          val named = f match {
+            case ne: NamedExpression => ne
+            case o => Alias(o, "propagatedFilter")()
+          }
+          mergedExpressions += named
+          named
+        }.toAttribute
+      }
+    }
+
+    val newPlanFilter = mergeFilter(newChildFilter)
+    val mergedPlanFilter = mergeFilter(mergedChildFilter)
+
+    (mergedExpressions.toSeq, newOutputMap, newPlanFilter, mergedPlanFilter)
+  }
+
+  // Only allow aggregates of the same implementation because merging different implementations
+  // could cause performance regression.
+  private def supportedAggregateMerge(newPlan: Aggregate, cachedPlan: Aggregate) = {
+    val newPlanAggregateExpressions = newPlan.aggregateExpressions.flatMap(_.collect {
+      case a: AggregateExpression => a
+    })
+    val cachedPlanAggregateExpressions = cachedPlan.aggregateExpressions.flatMap(_.collect {
+      case a: AggregateExpression => a
+    })
+    val newPlanSupportsHashAggregate = Aggregate.supportsHashAggregate(
+      newPlanAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes))
+    val cachedPlanSupportsHashAggregate = Aggregate.supportsHashAggregate(
+      cachedPlanAggregateExpressions.flatMap(_.aggregateFunction.aggBufferAttributes))
+    newPlanSupportsHashAggregate && cachedPlanSupportsHashAggregate ||
+      newPlanSupportsHashAggregate == cachedPlanSupportsHashAggregate && {
+        val newPlanSupportsObjectHashAggregate =
+          Aggregate.supportsObjectHashAggregate(newPlanAggregateExpressions)
+        val cachedPlanSupportsObjectHashAggregate =
+          Aggregate.supportsObjectHashAggregate(cachedPlanAggregateExpressions)
+        newPlanSupportsObjectHashAggregate && cachedPlanSupportsObjectHashAggregate ||
+          newPlanSupportsObjectHashAggregate == cachedPlanSupportsObjectHashAggregate
+      }
+  }
+
+  private def extractNonPropagatedFilter(e: Expression) = {
+    e match {
+      case And(e, _: PropagatedFilter) => Some(e)
+      case _: PropagatedFilter => None
+      case o => Some(o)
+    }
+  }
+
+  // We allow filter propagation into aggregates which:
+  // - doesn't have grouping expressions and
+  // - contains only the most basic aggregate functions.
+  private def supportsFilterPropagation(a: Aggregate) = {
+    a.groupingExpressions.isEmpty &&
+      a.aggregateExpressions.forall {
+        !_.exists {
+          case ae: AggregateExpression =>
+            ae.aggregateFunction match {
+              case _: Count | _: Sum | _: Average | _: Max | _: Min => false
+              case _ => true
+            }
+          case _ => false
+        }
+      }
+  }
+
+  private def filterAggregateExpressions(
+                                          aggregateExpressions: Seq[NamedExpression],
+                                          filter: Option[Expression]) = {
+    if (filter.isDefined) {
+      aggregateExpressions.map(_.transform {
+        case ae: AggregateExpression =>
+          ae.copy(filter = (ae.filter.toSeq :+ filter.get).reduceOption(And))
+      }.asInstanceOf[NamedExpression])
+    } else {
+      aggregateExpressions
+    }
+  }
+
+  private def removePropagatedFilters(plan: LogicalPlan) = {
+    plan.transformAllExpressions {
+      case pf: PropagatedFilter => pf.child
+    }
+  }
+
+  // Second traversal replaces `ScalarSubqueryReference`s to either
+  // `GetStructField(ScalarSubquery(CTERelationRef to the merged plan)` if the plan is merged from
+  // multiple subqueries or `ScalarSubquery(original plan)` if it isn't.
+  private def removeReferences(
+                                plan: LogicalPlan,
+                                cache: ArrayBuffer[Header]) = {
+    plan.transformUpWithSubqueries {
+      case n =>
+        n.transformExpressionsWithPruning(_.containsAnyPattern(SCALAR_SUBQUERY_REFERENCE)) {
+          case ssr: ScalarSubqueryReference =>
+            val header = cache(ssr.subqueryIndex)
+            if (header.merged) {
+              val subqueryCTE = header.plan.asInstanceOf[CTERelationDef]
+              GetStructField(
+                ScalarSubquery(
+                  CTERelationRef(subqueryCTE.id, _resolved = true, subqueryCTE.output),
+                  exprId = ssr.exprId),
+                ssr.headerIndex)
+            } else {
+              ScalarSubquery(header.plan, exprId = ssr.exprId)
+            }
+        }
+    }
+  }
+}
+
+/**
+ * Temporal reference to a subquery.
+ */
+case class ScalarSubqueryReference(
+                                    subqueryIndex: Int,
+                                    headerIndex: Int,
+                                    dataType: DataType,
+                                    exprId: ExprId) extends LeafExpression with Unevaluable {
+  override def nullable: Boolean = true
+
+  final override val nodePatterns: Seq[TreePattern] = Seq(SCALAR_SUBQUERY_REFERENCE)
+
+  override def stringArgs: Iterator[Any] = Iterator(subqueryIndex, headerIndex, dataType, exprId.id)
+}
+
+/**
+ * Temporal wrapper around already propagated predicates.
+ */
+case class PropagatedFilter(child: Expression) extends UnaryExpression with Unevaluable {
+  override def dataType: DataType = child.dataType
+
+  override protected def withNewChildInternal(newChild: Expression): PropagatedFilter =
+    copy(child = newChild)
+}
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala
index 6c6bd1799e..bd363b85b0 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/InjectRuntimeFilterSuite.scala
@@ -19,11 +19,11 @@ package org.apache.spark.sql
 
 import org.apache.spark.sql.catalyst.expressions.{Alias, BloomFilterMightContain, Literal}
 import org.apache.spark.sql.catalyst.expressions.aggregate.{AggregateExpression, BloomFilterAggregate}
-import org.apache.spark.sql.catalyst.optimizer.MergeScalarSubqueries
 import org.apache.spark.sql.catalyst.plans.LeftSemi
 import org.apache.spark.sql.catalyst.plans.logical.{Aggregate, Filter, Join, LogicalPlan}
 import org.apache.spark.sql.execution.{ReusedSubqueryExec, SubqueryExec}
 import org.apache.spark.sql.execution.adaptive.{AdaptiveSparkPlanHelper, AQEPropagateEmptyRelation}
+import org.apache.spark.sql.execution.merge.MergeScalarSubqueries
 import org.apache.spark.sql.internal.SQLConf
 import org.apache.spark.sql.test.{SharedSparkSession, SQLTestUtils}
 import org.apache.spark.sql.types.{IntegerType, StructType}
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/execution/merge/MergeScalarSubqueriesSuite.scala
similarity index 100%
rename from sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/MergeScalarSubqueriesSuite.scala
rename to sql/core/src/test/scala/org/apache/spark/sql/execution/merge/MergeScalarSubqueriesSuite.scala
-- 
2.20.1

